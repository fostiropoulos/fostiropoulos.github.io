<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Iordanis Fostiropoulos</title>

  <meta name="author" content="Iordanis Fostiropoulos">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ‘½</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Iordanis Fostiropoulos</name>
              </p>
              <p>I am a PhD Candidate at the <a href="https://viterbischool.usc.edu/">University of Southern California</a> advised by <a href="https://scholar.google.com/citations?user=xhUvqK8AAAAJ">Prof. Laurent Itti</a>, where I work on Machine Learning. I am the author of <a href="https://github.com/fostiropoulos/ablator">ABLATOR</a> a distributed PyTorch framework for Machine Learning experiments. On my 'free' time I lead a research group <a href="https://deep.usc.edu">deep.usc.edu</a> where I mentor MS students in writing better code and doing Deep Learning research. I have mentored three students into succesfully applying for a PhD program.
              </p>
              <p>
                On a previous life, I worked as a Software Engineer and currently I am breaking my head on why <a href="https://arxiv.org/abs/2106.03253">simpler models outperform Deep Learning models</a> at <a href="https://www.figure.com/">Figure</a> as a ML Engineer.
              </p>
              <p style="text-align:center">
                <a href="mailto:fostirop@usc.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=XTLv1v8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/fostiropoulos/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/IordanisFostiropoulos.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IordanisFostiropoulos.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
<!-- Prepare a container for your calendar. -->
<script
  src="https://cdn.rawgit.com/IonicaBizau/github-calendar/gh-pages/dist/github-calendar.min.js"
>
</script>

<!-- Optionally, include the theme (if you don't want to struggle to write the CSS) -->
<link
  rel="stylesheet"
  href="https://cdn.rawgit.com/IonicaBizau/github-calendar/gh-pages/dist/github-calendar.css"
/>

<heading>Git commit history</heading>
<br>
<!-- Prepare a container for your calendar. -->
<div class="calendar">
    <!-- Loading stuff -->

</div>

<script>
    new GitHubCalendar(".calendar", "fostiropoulos");
</script>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested on improving the Generalization of Machine Learning models. My work combines theoritical insights with pratical applications, where I have used ideas from Meta-Learning, Bayesian Inference, Information Theory to improve the performance of Machine Learning models on problems such as Continual Learning. A lot of my work is constantly in-progress.
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <b>Select Publications in Chronological Order</b>
            </td></tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ablator.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>ABLATOR: Robust Horizontal-Scaling of Machine Learning Ablation Experiments</papertitle>

                <br>
                <br><strong>Iordanis Fostiropoulos</strong>, Laurent Itti
                <br>
                <em>AutoML</em>, 2023
                <br>
                <a href="https://github.com/fostiropoulos/ablator">project page</a>
                /
                <a href="./data/ablator.pdf">paper</a>
                <p></p>
                <p>
                  Ablation experiments are important in improving Machine Learning (ML) models, where multiple experimental trials are required for each component of a ML model. We find lack of available tools and frameworks for horizontal scaling of ML experiments where manual effort is required that often lead to errors. We propose a statefull experiment design framework, ABLATOR, that can scale a single experiment to thousands of trials while being fault-tollerant and robust to errors. We performed the largest ablation experiment for tabular data on Transformer models to date, evaluating 2,337 models
                  in total. Finally, we open source ABLATOR;
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/rtml.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Trustworthy model evaluation on a budget</papertitle>

                <br>
                <br><strong>Iordanis Fostiropoulos</strong>, Bowman Brown, Laurent Itti
                <br>
                <em>ICLR</em>, 2023 RTML Workshop
                <br>
                <a href="https://github.com/fostiropoulos/trustml">project page</a>
                /
                <a href="https://openreview.net/pdf?id=_JXT98mOmR">paper</a>
                <p></p>
                <p>
                  We find that errors in the ablation setup can lead to incorrect explanations for which method components contribute to the performance. Using the insights from our meta-analysis, we demonstrate how current practices can lead to unreliable conclusions. We quantify the selection bias of Hyperparameter Optimization (HPO) strategies to show that only random sampling can produce reliable results when determining the top and mean performance of a method under a limited computational budget.
                </p>
              </td>
            </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/bmc.png' width="160">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Batch Model Consolidation: A Multi-Task Model Consolidation Framework</papertitle>

        <br>
        <br>
        <strong>Iordanis Fostiropoulos</strong>, Jiaye Zhu, Laurent Itti
        <br>
        <em>CVPR</em>, 2023
        <br>
        <a href="https://fostiropoulos.github.io/stream_benchmark">project page</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf">paper</a>
        <p></p>
        <p>
        We propose a method to incrementally learn new tasks for a 'base model' using multiple learners and in a distributed fashion. We consolidate the 'knoweldge' of several learners at larger incremental steps that lead to reduced catastrophic forgetting when compared to smaller steps. We find simpler methods to avoid catastrophic forgetting outperform current state-of-the-art in our challenging Stream Benchmark.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/skill.png' width="160">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Lightweight Learner for Shared Knowledge Lifelong Learning</papertitle>

        <br>
        <br>
        Yunhao Ge, Yuecheng Li, Di Wu, Ao Xu, Adam M Jones, Amanda Sofie Rios, <strong>Iordanis Fostiropoulos</strong>, Shixian Wen, Po-Hsuan Huang, Zachary William Murdock, Gozde Sahin, Shuo Ni, Kiran Lekkala, Sumedh Anand Sontakke, Laurent Itti
        <br>
        <em>TMLR</em>, 2023
        <br>
        <a href="https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning">project page</a>
        /
        <a href="https://arxiv.org/pdf/2305.15591.pdf">paper</a>
        <p></p>
        <p>
          We propose a new Shared Knowledge Lifelong Learning (SKILL) challenge, which deploys a decentralized population of LL agents that each sequentially learn different tasks, with all agents operating independently and in parallel. After learning their respective tasks, agents share and consolidate their knowledge over a decentralized communication network, so that, in the end, all agents can master all tasks.         </p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/dq.png' width="160">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Implicit Feature Decoupling with Depthwise Quantization</papertitle>

        <br>
        <br>
        <strong>Iordanis Fostiropoulos</strong>, Barry Boehm
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://github.com/fostiropoulos/Depthwise-Quantization">project page</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fostiropoulos_Implicit_Feature_Decoupling_With_Depthwise_Quantization_CVPR_2022_paper.pdf">paper</a>
        <p></p>
        <p>
          We propose a new Vector Quantization method for latent features that improves reconstruction performance on images. By decomposing the features in the latent space and learning an auto-encoder end-to-end we show that it implicitly decouples the features and leads to improved reconstruction performance.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/spt.jpg' width="160">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Multimodal Phased Transformer for Sentiment Analysis</papertitle>

        <br>
        <br>
        Junyan Cheng*, <strong>Iordanis Fostiropoulos*</strong>, Barry Boehm, Mohammad Soleymani
        <br>
        <em>EMNLP</em>, 2021

        <br>
        * Equal Constribution
        <br>
        <a href="https://github.com/chengjunyan1/sp-transformer">project page</a>
        /
        <a href="https://aclanthology.org/2021.emnlp-main.189.pdf">paper</a>
        <p></p>
        <p>
          The quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/hyp.png' width="160">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Learning Hyperbolic Representations of Topological Features</papertitle>

        <br>
        <br>
        Panagiotis Kyriakis, <strong>Iordanis Fostiropoulos</strong>, Paul Bogdan
        <br>
        <em>ICLR</em>, 2021

        <br>
        <br>
        <a href="https://github.com/pkyriakis/permanifold">project page</a>
        /
        <a href="https://openreview.net/pdf?id=yqPnIRhHtZv">paper</a>
        <p></p>
        <p>
          We propose a method to learn representations of persistence diagrams on hyperbolic spaces, more specifically on the Poincare ball. By representing features of infinite persistence infinitesimally close to the boundary of the ball, their distance to non-essential features approaches infinity, thereby their relative importance is preserved. </p>
      </td>
    </tr>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
              <heading>Talks</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/transformers.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/transformers.pdf">
                <papertitle>Transformers And Beyond</papertitle>
              </a>
              <br>
              <strong>Iordanis Fostiropoulos</strong>
              <br>
              <em>April 4th</em>, 2023
              <br>
              <p>
                A summary of Transformer models given at 221 students at University of Southern California as part of their Artificial Intelligence curriculum (CSCI561)
              </p>
            </td>
          </tr>



        </tbody></table>
      </td>
    </tr>
  </table>
</body>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6NJ9DNYY2X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6NJ9DNYY2X');
</script>
</html>
