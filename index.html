<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Iordanis Fostiropoulos</title>

  <meta name="author" content="Iordanis Fostiropoulos">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Optionally, include the theme (if you don't want to struggle to write the CSS) -->
  <link rel="stylesheet"
    href="https://cdn.rawgit.com/IonicaBizau/github-calendar/gh-pages/dist/github-calendar.css" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ‘½</text></svg>">
</head>

<body>
  <table class="main">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table class="section">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Iordanis Fostiropoulos</name>
                  </p>
                  <p>I am a PhD Candidate at the <a href="https://viterbischool.usc.edu/">University of Southern
                      California</a> advised by <a href="https://scholar.google.com/citations?user=xhUvqK8AAAAJ">Prof.
                      Laurent Itti</a>, where I work on Machine Learning. I am the author of <a
                      href="https://github.com/fostiropoulos/ablator">ABLATOR</a> a distributed PyTorch framework for
                    Machine Learning experiments. On my 'free' time I lead a research group <a
                      href="https://deep.usc.edu">deep.usc.edu</a> where I mentor MS students in writing better code and
                    doing Deep Learning research. I have mentored three students into succesfully applying for a PhD
                    program.
                  </p>
                  <p>
                    On a previous life, I worked as a Software Engineer and currently I am breaking my head on why <a
                      href="https://arxiv.org/abs/2106.03253">simpler models outperform Deep Learning models</a> at <a
                      href="https://www.figure.com/">Figure</a> as a ML Engineer.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:fostirop@usc.edu">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=XTLv1v8AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/fostiropoulos/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/IordanisFostiropoulos.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/IordanisFostiropoulos.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="section">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <!-- Prepare a container for your calendar. -->
                  <script src="https://cdn.rawgit.com/IonicaBizau/github-calendar/gh-pages/dist/github-calendar.min.js">
                  </script>


                  <heading>Git commit Trophies</heading>
                  <br>
                  <!-- Prepare a container for your calendar. -->


                  <img src='https://streak-stats.demolab.com?user=fostiropoulos' width="100%">
                  <img src='https://github-profile-trophy.vercel.app/?username=fostiropoulos&theme=flat' width="100%">

                </td>
              </tr>
            </tbody>
          </table>

          <table class="section">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested on improving the Generalization of Machine Learning models. My work combines
                    theoritical insights with pratical applications, where I have used ideas from Meta-Learning,
                    Bayesian Inference, Information Theory to improve the performance of Machine Learning models on
                    problems such as Continual Learning. A lot of my work is constantly in-progress.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="section">
            <tbody>


              <tr>
                <td colspan="2" class="thumbnail">
                  <heading>Select Publications in Chronological Order</heading>
                </td>
              </tr>

              <tr>
                <td class="thumbnail">

                  <img src='images/regpt.png' width="160">
                </td>
                <td class="paper">
                  <papertitle>Probing Reasoning of Language Models with Inductive In-Context Learning</papertitle>

                  <br>
                  <br><strong>Iordanis Fostiropoulos</strong>, Laurent Itti
                  <br>
                  <em>ICJAI</em> (oral), 2023 Workshop on Knowledge-Based Compositional Generalization
                  <br>
                  <a href="https://github.com/fostiropoulos/ReAnalogy">project page</a>
                  /
                  <a href="https://openreview.net/pdf?id=skvqz58ys1U">paper</a>
                  <p></p>
                  <p>
                    Previous work evaluate the reasoning of Language Models (LMs) using tests that can be inapplicable to a LM. In this work we propose an unbiased and challenging evaluation setting using complex Regular expressions composed of Quasi-Natural Language. We evaluate the inductive reasoning ability of a LM to generate Facts that abide by the Rules. We find that when a Rule is injected in the training sequence of a LM it learn to associate the Facts with the Rules implicitly. When the same model is probed with Inductive In-Context Learning, where the same model is first probed to generate a sound Rule, it will generate probable Facts.
                  </p>
                </td>
              </tr>
              <tr>
                <td class="thumbnail">

                  <img src='images/ablator.png' width="160">
                </td>
                <td class="paper">
                  <papertitle>ABLATOR: Robust Horizontal-Scaling of Machine Learning Ablation Experiments</papertitle>

                  <br>
                  <br><strong>Iordanis Fostiropoulos</strong>, Laurent Itti
                  <br>
                  <em>AutoML</em>, 2023
                  <br>
                  <a href="https://github.com/fostiropoulos/ablator">project page</a>
                  /
                  <a href="./data/ablator.pdf">paper</a>
                  <p></p>
                  <p>
                    Ablation experiments are important in improving Machine Learning (ML) models, where multiple
                    experimental trials are required for each component of a ML model. We find lack of available tools
                    and frameworks for horizontal scaling of ML experiments where manual effort is required that often
                    lead to errors. We propose a statefull experiment design framework, ABLATOR, that can scale a single
                    experiment to thousands of trials while being fault-tollerant and robust to errors. We performed the
                    largest ablation experiment for tabular data on Transformer models to date, evaluating 2,337 models
                    in total. Finally, we open source ABLATOR;
                  </p>
                </td>
              </tr>

              <tr>
                <td class="thumbnail">

                  <img src='images/rtml.png' width="160">
                  </div>
                </td>
                <td class="paper">
                  <papertitle>Trustworthy model evaluation on a budget</papertitle>

                  <br>
                  <br><strong>Iordanis Fostiropoulos</strong>, Bowman Brown, Laurent Itti
                  <br>
                  <em>ICLR</em>, 2023 RTML Workshop
                  <br>
                  <a href="https://github.com/fostiropoulos/trustml">project page</a>
                  /
                  <a href="https://openreview.net/pdf?id=_JXT98mOmR">paper</a>
                  <p></p>
                  <p>
                    We find that errors in the ablation setup can lead to incorrect explanations for which method
                    components contribute to the performance. Using the insights from our meta-analysis, we demonstrate
                    how current practices can lead to unreliable conclusions. We quantify the selection bias of
                    Hyperparameter Optimization (HPO) strategies to show that only random sampling can produce reliable
                    results when determining the top and mean performance of a method under a limited computational
                    budget.
                  </p>
                </td>
              </tr>

              <tr>
                <td class="thumbnail">
                  <img src='images/bmc.png' width="160">
                </td>
                <td class="paper">
                  <papertitle>Batch Model Consolidation: A Multi-Task Model Consolidation Framework</papertitle>

                  <br>
                  <br>
                  <strong>Iordanis Fostiropoulos</strong>, Jiaye Zhu, Laurent Itti
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a href="https://fostiropoulos.github.io/stream_benchmark">project page</a>
                  /
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf">paper</a>
                  <p></p>
                  <p>
                    We propose a method to incrementally learn new tasks for a 'base model' using multiple learners and
                    in a distributed fashion. We consolidate the 'knoweldge' of several learners at larger incremental
                    steps that lead to reduced catastrophic forgetting when compared to smaller steps. We find simpler
                    methods to avoid catastrophic forgetting outperform current state-of-the-art in our challenging
                    Stream Benchmark.
                  </p>
                </td>
              </tr>

              <tr>
                <td class="thumbnail">

                  <img src='images/skill.png' width="160">
                </td>
                <td class="paper">
                  <papertitle>Lightweight Learner for Shared Knowledge Lifelong Learning</papertitle>

                  <br>
                  <br>
                  Yunhao Ge, Yuecheng Li, Di Wu, Ao Xu, Adam M Jones, Amanda Sofie Rios, <strong>Iordanis
                    Fostiropoulos</strong>, Shixian Wen, Po-Hsuan Huang, Zachary William Murdock, Gozde Sahin, Shuo Ni,
                  Kiran Lekkala, Sumedh Anand Sontakke, Laurent Itti
                  <br>
                  <em>TMLR</em>, 2023
                  <br>
                  <a href="https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2305.15591.pdf">paper</a>
                  <p></p>
                  <p>
                    We propose a new Shared Knowledge Lifelong Learning (SKILL) challenge, which deploys a decentralized
                    population of LL agents that each sequentially learn different tasks, with all agents operating
                    independently and in parallel. After learning their respective tasks, agents share and consolidate
                    their knowledge over a decentralized communication network, so that, in the end, all agents can
                    master all tasks. </p>
                </td>
              </tr>
              <tr>
                <td class="thumbnail">

                  <img src='images/dq.png' width="160">
                </td>
                <td class="paper">
                  <papertitle>Implicit Feature Decoupling with Depthwise Quantization</papertitle>

                  <br>
                  <br>
                  <strong>Iordanis Fostiropoulos</strong>, Barry Boehm
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://github.com/fostiropoulos/Depthwise-Quantization">project page</a>
                  /
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fostiropoulos_Implicit_Feature_Decoupling_With_Depthwise_Quantization_CVPR_2022_paper.pdf">paper</a>
                  <p></p>
                  <p>
                    We propose a new Vector Quantization method for latent features that improves reconstruction
                    performance on images. By decomposing the features in the latent space and learning an auto-encoder
                    end-to-end we show that it implicitly decouples the features and leads to improved reconstruction
                    performance.</p>
                </td>
              </tr>
              <tr>
                <td class="thumbnail">

                  <img src='images/spt.jpg' width="160">
                </td>
                <td class="paper">
                  <papertitle>Multimodal Phased Transformer for Sentiment Analysis</papertitle>

                  <br>
                  <br>
                  Junyan Cheng*, <strong>Iordanis Fostiropoulos*</strong>, Barry Boehm, Mohammad Soleymani
                  <br>
                  <em>EMNLP</em>, 2021

                  <br>
                  * Equal Constribution
                  <br>
                  <a href="https://github.com/chengjunyan1/sp-transformer">project page</a>
                  /
                  <a href="https://aclanthology.org/2021.emnlp-main.189.pdf">paper</a>
                  <p></p>
                  <p>
                    The quadratic complexity of the self-attention mechanism in Transformers limits their deployment in
                    low-resource devices and makes their inference and training computationally expensive. We propose
                    multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and
                    memory footprint. </p>
                </td>
              </tr>

              <tr>
                <td class="thumbnail">
                  <img src='images/hyp.png' width="160">
                </td>
                <td class="paper">
                  <papertitle>Learning Hyperbolic Representations of Topological Features</papertitle>

                  <br>
                  <br>
                  Panagiotis Kyriakis, <strong>Iordanis Fostiropoulos</strong>, Paul Bogdan
                  <br>
                  <em>ICLR</em>, 2021
                  <br>
                  <br>
                  <a href="https://github.com/pkyriakis/permanifold">project page</a>
                  /
                  <a href="https://openreview.net/pdf?id=yqPnIRhHtZv">paper</a>
                  <p></p>
                  <p>
                    We propose a method to learn representations of persistence diagrams on hyperbolic spaces, more
                    specifically on the Poincare ball. By representing features of infinite persistence infinitesimally
                    close to the boundary of the ball, their distance to non-essential features approaches infinity,
                    thereby their relative importance is preserved. </p>
                </td>
              </tr>


            </tbody>
          </table>
          <table class="section">
            <tbody>
              <tr>
                <td>
                  <heading>Talks</heading>
                </td>
              </tr>
              <tr>
                <td class="thumbnail">

                  <img src='images/transformers.jpg' width="160">
                  </div>
                </td>
                <td class="paper">
                  <a href="data/transformers.pdf">
                    <papertitle>Transformers And Beyond</papertitle>
                  </a>
                  <br>
                  <strong>Iordanis Fostiropoulos</strong>
                  <br>
                  <em>April 4th</em>, 2023
                  <br>
                  <p>
                    A summary of Transformer models given at 221 students at University of Southern California as part
                    of their Artificial Intelligence curriculum (CSCI561)
                  </p>
                </td>
              </tr>



            </tbody>
          </table>
          <table class="section">
            <tbody>
              <tr>
                <td>
                  <heading>Service</heading>
                  <p>
                    Served as a reviewer for CVPR, ICCV, NeurIPS, AAAI, AutoML, IJCAI, AISTATS
                  </p>
                </td>
              </tr>


            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6NJ9DNYY2X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-6NJ9DNYY2X');
</script>

</html>